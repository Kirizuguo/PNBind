"""
PNBind - å±€éƒ¨Cross-Attentionç‰ˆæœ¬
é€šè¿‡k-NNçº¦æŸattentionæ„Ÿå—é‡ï¼Œä¿ç•™å‡ ä½•å±€éƒ¨æ€§
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from torch_geometric.nn import PointGNNConv, global_mean_pool as gep

# ====================================================================
# åŸºç¡€æ¨¡å—ï¼ˆå¤ç”¨ï¼‰
# ====================================================================

class mlp_h(nn.Module):
    """PointGNN ä¸­çš„ h_theta MLP"""
    def __init__(self, inputdim=1536, hiddendim=768, outputdim=1536, dropout=0.2):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(inputdim, hiddendim),
            nn.LayerNorm(hiddendim),
            nn.Dropout(dropout),
            nn.LeakyReLU(),
            nn.Linear(hiddendim, outputdim),
            nn.Dropout(dropout),
            nn.LeakyReLU()
        )
    
    def forward(self, x):
        return self.mlp(x)


class mlp_f(nn.Module):
    """PointGNN ä¸­çš„ f_theta MLP"""
    def __init__(self, inputdim=3072, hiddendim=1536, outputdim=1536, dropout=0.2):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(inputdim, hiddendim),
            nn.LayerNorm(hiddendim),
            nn.Dropout(dropout),
            nn.LeakyReLU(),
            nn.Linear(hiddendim, outputdim),
            nn.Dropout(dropout),
            nn.LeakyReLU()
        )
    
    def forward(self, x):
        return self.mlp(x)


class mlp_g(nn.Module):
    """PointGNN ä¸­çš„ g_theta MLP"""
    def __init__(self, inputdim=1536, hiddendim=1536, outputdim=1536, dropout=0.2):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(inputdim, hiddendim),
            nn.LayerNorm(hiddendim),
            nn.Dropout(dropout),
            nn.LeakyReLU(),
            nn.Linear(hiddendim, outputdim),
            nn.Dropout(dropout),
            nn.LeakyReLU()
        )
    
    def forward(self, x):
        return self.mlp(x)


# ====================================================================
# ğŸ”´ æ ¸å¿ƒæ”¹è¿›ï¼šå±€éƒ¨Cross-Attention with Gated Residual
# ====================================================================

class LocalCrossModalAttention(nn.Module):
    """
    å±€éƒ¨åŒ–Cross-Attentionèåˆ
    
    å…³é”®æ”¹è¿›ï¼š
    1. k-NN mask: åªåœ¨å±€éƒ¨é‚»åŸŸå†…è®¡ç®—attention
    2. Gated Residual: å¯å­¦ä¹ çš„æ®‹å·®æƒé‡
    3. ä¿ç•™å‡ ä½•å±€éƒ¨æ€§
    """
    def __init__(self, geo_dim=1536, sem_dim=512, rsa_dim=64, 
                 output_dim=256, num_heads=8, k_neighbors=10, dropout=0.2):
        super().__init__()
        
        self.output_dim = output_dim
        self.num_heads = num_heads
        self.head_dim = output_dim // num_heads
        self.k = k_neighbors
        
        # æŠ•å½±å±‚
        self.geo_proj = nn.Sequential(
            nn.Linear(geo_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.Dropout(dropout)
        )
        
        self.sem_proj = nn.Sequential(
            nn.Linear(sem_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.Dropout(dropout)
        )
        
        self.rsa_proj = nn.Sequential(
            nn.Linear(rsa_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.Dropout(dropout)
        )
        
        # Multi-head attention components
        self.q_proj = nn.Linear(output_dim, output_dim)
        self.k_proj = nn.Linear(output_dim, output_dim)
        self.v_proj = nn.Linear(output_dim, output_dim)
        self.out_proj = nn.Linear(output_dim, output_dim)
        
        # ğŸ”´ Gated Residual å¯å­¦ä¹ æƒé‡
        self.alpha_geo = nn.Parameter(torch.tensor(1.0))
        self.beta_geo = nn.Parameter(torch.tensor(0.5))
        self.alpha_sem = nn.Parameter(torch.tensor(1.0))
        self.beta_sem = nn.Parameter(torch.tensor(0.5))
        
        # RSAç‹¬ç«‹å¤„ç†
        self.rsa_transform = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.LeakyReLU()
        )
        
        # æœ€ç»ˆèåˆ
        self.fusion = nn.Sequential(
            nn.Linear(output_dim * 3, output_dim * 2),
            nn.LayerNorm(output_dim * 2),
            nn.Dropout(dropout),
            nn.LeakyReLU(),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim),
            nn.LeakyReLU()
        )
        
        self.layer_norm1 = nn.LayerNorm(output_dim)
        self.layer_norm2 = nn.LayerNorm(output_dim)
        
        self.dropout = nn.Dropout(dropout)
    
    def build_knn_mask(self, pos, k):
        """
        æ„å»ºk-NNé‚»åŸŸmask
        
        Args:
            pos: [L, 3] 3Dåæ ‡
            k: int, é‚»å±…æ•°é‡
        
        Returns:
            mask: [L, L] bool tensor, Trueè¡¨ç¤ºå¯ä»¥attend
        """
        L = pos.size(0)
        
        # è®¡ç®—æ¬§æ°è·ç¦»
        dist = torch.cdist(pos, pos)  # [L, L]
        
        # æ‰¾åˆ°æœ€è¿‘çš„kä¸ªé‚»å±…ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
        _, indices = torch.topk(dist, min(k, L), largest=False, dim=1)
        
        # æ„å»ºmask
        mask = torch.zeros(L, L, dtype=torch.bool, device=pos.device)
        mask.scatter_(1, indices, True)
        
        return mask
    
    def local_attention(self, query, key, value, mask):
        """
        å±€éƒ¨masked attention
        
        Args:
            query, key, value: [L, num_heads, head_dim]
            mask: [L, L] bool mask
        
        Returns:
            output: [L, num_heads, head_dim]
        """
        # ğŸ”´ è½¬æ¢ç»´åº¦: [L, num_heads, head_dim] -> [num_heads, L, head_dim]
        query = query.transpose(0, 1)  # [num_heads, L, head_dim]
        key = key.transpose(0, 1)      # [num_heads, L, head_dim]
        value = value.transpose(0, 1)  # [num_heads, L, head_dim]
        
        # Scaled dot-product attention
        scores = torch.matmul(query, key.transpose(-2, -1))  # [num_heads, L, L]
        scores = scores / math.sqrt(self.head_dim)
        
        # ğŸ”´ åº”ç”¨k-NN maskï¼ˆè¿œè·ç¦»è®¾ä¸º-infï¼‰
        if mask is not None:
            # mask: [L, L] -> [1, L, L] -> broadcast to [num_heads, L, L]
            mask = mask.unsqueeze(0)  # [1, L, L]
            scores = scores.masked_fill(~mask, float('-inf'))
        
        # Softmax + dropout
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # Apply attention
        output = torch.matmul(attn_weights, value)  # [num_heads, L, head_dim]
        
        # ğŸ”´ è½¬æ¢å›æ¥: [num_heads, L, head_dim] -> [L, num_heads, head_dim]
        output = output.transpose(0, 1)
        
        return output
    
    def forward(self, geo_feat, sem_feat, rsa_feat, pos):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            geo_feat: [L, 1536] å‡ ä½•ç‰¹å¾
            sem_feat: [L, 512] è¯­ä¹‰ç‰¹å¾
            rsa_feat: [L, 64] RSAç‰¹å¾
            pos: [L, 3] 3Dåæ ‡ï¼ˆç”¨äºæ„å»ºk-NNï¼‰
        
        Returns:
            fused: [L, 256] èåˆç‰¹å¾
        """
        L = geo_feat.size(0)
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        geo_proj = self.geo_proj(geo_feat)  # [L, 256]
        sem_proj = self.sem_proj(sem_feat)  # [L, 256]
        rsa_proj = self.rsa_proj(rsa_feat)  # [L, 256]
        
        # ğŸ”´ æ„å»ºk-NN maskï¼ˆåŸºäº3Dåæ ‡ï¼‰
        knn_mask = self.build_knn_mask(pos, self.k)  # [L, L]
        
        # ============================================================
        # å‡ ä½• attend to è¯­ä¹‰ï¼ˆå±€éƒ¨ï¼‰
        # ============================================================
        geo_q = self.q_proj(geo_proj).view(L, self.num_heads, self.head_dim)
        sem_k = self.k_proj(sem_proj).view(L, self.num_heads, self.head_dim)
        sem_v = self.v_proj(sem_proj).view(L, self.num_heads, self.head_dim)
        
        geo_attn_out = self.local_attention(geo_q, sem_k, sem_v, knn_mask)
        geo_attn_out = geo_attn_out.reshape(L, self.output_dim)
        geo_attn_out = self.out_proj(geo_attn_out)
        
        # ğŸ”´ Gated Residualèåˆ
        geo_output = self.layer_norm1(
            self.alpha_geo * geo_proj + self.beta_geo * geo_attn_out
        )
        
        # ============================================================
        # è¯­ä¹‰ attend to å‡ ä½•ï¼ˆå±€éƒ¨ï¼‰
        # ============================================================
        sem_q = self.q_proj(sem_proj).view(L, self.num_heads, self.head_dim)
        geo_k = self.k_proj(geo_proj).view(L, self.num_heads, self.head_dim)
        geo_v = self.v_proj(geo_proj).view(L, self.num_heads, self.head_dim)
        
        sem_attn_out = self.local_attention(sem_q, geo_k, geo_v, knn_mask)
        sem_attn_out = sem_attn_out.reshape(L, self.output_dim)
        sem_attn_out = self.out_proj(sem_attn_out)
        
        # ğŸ”´ Gated Residualèåˆ
        sem_output = self.layer_norm2(
            self.alpha_sem * sem_proj + self.beta_sem * sem_attn_out
        )
        
        # ============================================================
        # RSAå•ç‹¬å¤„ç†
        # ============================================================
        rsa_output = self.rsa_transform(rsa_proj)
        
        # ============================================================
        # æœ€ç»ˆèåˆä¸‰ä¸ªæ¨¡æ€
        # ============================================================
        combined = torch.cat([geo_output, sem_output, rsa_output], dim=1)
        fused = self.fusion(combined)
        
        return fused
# ====================================================================
# T-Net
# ====================================================================

class Tnet(nn.Module):
    """T-Netç”¨äºåæ ‡çš„æ—‹è½¬/å¹³ç§»ä¸å˜æ€§"""
    def __init__(self, input_dim=3):
        super(Tnet, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 1024)
        self.fc4 = nn.Linear(1024, 512)
        self.fc5 = nn.Linear(512, 256)
        self.fc6 = nn.Linear(256, 9)
        
        self.act = nn.LeakyReLU()
        self.ln1 = nn.LayerNorm(64)
        self.ln2 = nn.LayerNorm(128)
        self.ln3 = nn.LayerNorm(1024)
        self.ln4 = nn.LayerNorm(512)
        self.ln5 = nn.LayerNorm(256)
    
    def forward(self, x):
        x = self.act(self.ln1(self.fc1(x)))
        x = self.act(self.ln2(self.fc2(x)))
        x = self.act(self.ln3(self.fc3(x)))
        
        batch = torch.zeros(x.shape[0], dtype=torch.long, device=x.device)
        x = gep(x, batch=batch)
        
        x = self.act(self.ln4(self.fc4(x)))
        x = self.act(self.ln5(self.fc5(x)))
        x = self.fc6(x)
        
        iden = torch.eye(3, dtype=torch.float32, device=x.device).view(-1)
        x = x + iden
        return x.view(3, 3)


# ====================================================================
# ä½ç½®ç¼–ç 
# ====================================================================

def positional_encoding(d_model, max_len=7000):
    """æ ‡å‡†Transformerä½ç½®ç¼–ç """
    position = torch.arange(max_len).unsqueeze(1).float()
    div_term = torch.exp(
        torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model)
    )
    pe = torch.zeros((max_len, d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    return pe


# ====================================================================
# ä¸»æ¨¡å‹
# ====================================================================

class benchmark(nn.Module):
    """
    PNBind - å±€éƒ¨Cross-Attentionç‰ˆæœ¬
    
    å…³é”®æ”¹è¿›ï¼š
    1. å±€éƒ¨k-NNçº¦æŸçš„Cross-Attention
    2. å¯å­¦ä¹ çš„Gated Residual
    3. ä¿ç•™GNNçš„å±€éƒ¨å‡ ä½•ç‰¹å¾
    """
    
    def __init__(self, esm_dim=1536, num_classes=2, dropout_rate=0.2, k_neighbors=10):
        super().__init__()
        
        self.esm_dim = esm_dim
        self.k_neighbors = k_neighbors
        self.dropout = nn.Dropout(dropout_rate)
        self.LeakyReLU = nn.LeakyReLU()
        
        # ================================================================
        # å‡ ä½•åˆ†æ”¯
        # ================================================================
        self.pos_tnet = Tnet(input_dim=3)
        self.pos_proj = nn.Linear(3, esm_dim)
        
        self.gnn_layers = nn.ModuleList([
            PointGNNConv(
                mlp_h(esm_dim, esm_dim//2, esm_dim, dropout_rate),
                mlp_f(esm_dim*2, esm_dim, esm_dim, dropout_rate),
                mlp_g(esm_dim, esm_dim, esm_dim, dropout_rate)
            )
            for _ in range(3)
        ])
        
        # ================================================================
        # è¯­ä¹‰åˆ†æ”¯
        # ================================================================
        semantic_input_dim = esm_dim + 30 + 20 + 13  # 1599
        semantic_output_dim = 512
        
        self.semantic_branch = nn.Sequential(
            nn.Linear(semantic_input_dim, 1024),
            nn.LayerNorm(1024),
            nn.Dropout(dropout_rate),
            nn.LeakyReLU(),
            nn.Linear(1024, 768),
            nn.LayerNorm(768),
            nn.Dropout(dropout_rate),
            nn.LeakyReLU(),
            nn.Linear(768, semantic_output_dim),
            nn.LayerNorm(semantic_output_dim),
            nn.Dropout(dropout_rate),
            nn.LeakyReLU()
        )
        
        # ================================================================
        # RSAåˆ†æ”¯
        # ================================================================
        rsa_output_dim = 64
        self.rsa_branch = nn.Sequential(
            nn.Linear(1, 32),
            nn.LayerNorm(32),
            nn.LeakyReLU(),
            nn.Linear(32, rsa_output_dim),
            nn.LayerNorm(rsa_output_dim),
            nn.LeakyReLU()
        )
        
        # ================================================================
        # ğŸ”´ å±€éƒ¨Cross-Attentionèåˆå±‚
        # ================================================================
        fusion_output_dim = 256
        
        self.fusion_layer = LocalCrossModalAttention(
            geo_dim=esm_dim,
            sem_dim=semantic_output_dim,
            rsa_dim=rsa_output_dim,
            output_dim=fusion_output_dim,
            num_heads=8,
            k_neighbors=k_neighbors,
            dropout=dropout_rate
        )
        
        # ================================================================
        # Transformer Decoder
        # ================================================================
        self.decoder_layer = nn.TransformerDecoderLayer(
            d_model=fusion_output_dim,
            nhead=8,
            dim_feedforward=1024,
            dropout=dropout_rate,
            activation='gelu'
        )
        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=2)
        
        # ================================================================
        # åˆ†ç±»å¤´
        # ================================================================
        self.classifier = nn.Sequential(
            nn.Linear(fusion_output_dim, 128),
            nn.LeakyReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, num_classes)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, point_graph):
        """å‰å‘ä¼ æ’­"""
        # æå–è¾“å…¥
        edge_index = point_graph.edge_index
        pos = point_graph.pos
        x_esm = point_graph.x_esm3
        hmm = point_graph.hmm
        pssm = point_graph.pssm
        secondary = point_graph.secondary
        asa = point_graph.asa
        
        # é•¿åº¦å¯¹é½
        min_len = min(
            pos.size(0), x_esm.size(0), hmm.size(0),
            pssm.size(0), secondary.size(0), asa.size(0)
        )
        
        pos = pos[:min_len]
        x_esm = x_esm[:min_len]
        hmm = hmm[:min_len]
        pssm = pssm[:min_len]
        secondary = secondary[:min_len]
        asa = asa[:min_len]
        edge_mask = (edge_index[0] < min_len) & (edge_index[1] < min_len)
        edge_index = edge_index[:, edge_mask]
        
        # ============================================================
        # å‡ ä½•åˆ†æ”¯
        # ============================================================
        # åæ ‡æ ‡å‡†åŒ–
        pos_centered = pos - pos.mean(dim=0, keepdim=True)
        pos_normalized = pos_centered / (pos_centered.std(dim=0, keepdim=True) + 1e-6)
        
        # T-Netå¯¹é½
        T = self.pos_tnet(pos_normalized)
        pos_transformed = torch.matmul(pos_normalized, T)
        
        # æŠ•å½± + GNN
        pos_embed = self.pos_proj(F.layer_norm(pos_transformed, (3,)))
        
        geo_feat = pos_embed
        for layer in self.gnn_layers:
            geo_feat = self.LeakyReLU(layer(geo_feat, pos_embed, edge_index))
        
        # ============================================================
        # è¯­ä¹‰åˆ†æ”¯
        # ============================================================
        semantic_input = torch.cat([x_esm, hmm, pssm, secondary], dim=1)
        semantic_feat = self.semantic_branch(semantic_input)
        
        # ============================================================
        # RSAåˆ†æ”¯
        # ============================================================
        rsa_feat = self.rsa_branch(asa.unsqueeze(1))
        
        # ============================================================
        # ğŸ”´ å±€éƒ¨Cross-Attentionèåˆï¼ˆä¼ å…¥posç”¨äºk-NNï¼‰
        # ============================================================
        fused = self.fusion_layer(geo_feat, semantic_feat, rsa_feat, pos_normalized)
        
        # ============================================================
        # Transformer Decoder
        # ============================================================
        pe = positional_encoding(fused.size(1), min_len).to(fused.device)
        fused_with_pe = fused + pe[:min_len, :fused.size(1)]
        
        fused_t = fused_with_pe.unsqueeze(1)
        decoded = self.decoder(fused_t, fused_t).squeeze(1)
        
        # ============================================================
        # åˆ†ç±»
        # ============================================================
        out = self.classifier(decoded)
        
        return out, min_len