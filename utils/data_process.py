# data_process.py
_esm3_model_cache = None
_esmc_model_cache = None
_esm2_model_cache = None      # æ–°å¢
_esm2_alphabet_cache = None   # æ–°å¢
_esm1b_model_cache = None      # æ–°å¢
_esm1b_alphabet_cache = None   # æ–°å¢
_esm2_3b_model_cache = None    # æ–°å¢
_esm2_3b_alphabet_cache = None # æ–°å¢
import sys, os
import matplotlib.pyplot as plt

def analyze_sequence_lengths():
    """ç»Ÿè®¡æ•°æ®é›†ä¸­çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ"""
    lengths = []
    
    for dataset, split in [("prna", "train"), ("prna", "test"), ("pdna", "train"), ("pdna", "test")]:
        label_dir = f"{dataset}_labels/{dataset}_{split}_label_onlyc"
        if not os.path.exists(label_dir):
            continue
            
        for file in os.listdir(label_dir):
            with open(os.path.join(label_dir, file), 'r') as f:
                lines = f.readlines()
                lengths.append(len(lines))
    
    lengths = sorted(lengths)
    print(f"\n{'='*60}")
    print(f"åºåˆ—é•¿åº¦ç»Ÿè®¡ (æ€»è®¡{len(lengths)}ä¸ªæ ·æœ¬)")
    print(f"{'='*60}")
    print(f"æœ€çŸ­: {min(lengths)}")
    print(f"æœ€é•¿: {max(lengths)}")
    print(f"å¹³å‡: {sum(lengths)/len(lengths):.1f}")
    print(f"ä¸­ä½æ•°: {lengths[len(lengths)//2]}")
    print(f"\nè¶…è¿‡1024çš„æ ·æœ¬æ•°: {sum(1 for l in lengths if l > 1024)} ({sum(1 for l in lengths if l > 1024)/len(lengths)*100:.1f}%)")
    print(f"è¶…è¿‡2048çš„æ ·æœ¬æ•°: {sum(1 for l in lengths if l > 2048)} ({sum(1 for l in lengths if l > 2048)/len(lengths)*100:.1f}%)")
    
    return lengths

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
print("[PATH FIX] Added:", os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import numpy as np
import os
from torch_geometric.data import InMemoryDataset
from torch_geometric.data import Data
import torch
from torch_cluster import radius_graph
import json
from tqdm import tqdm
from transformers import T5EncoderModel, T5Tokenizer, pipeline, AlbertModel, AlbertTokenizer, XLNetTokenizer, XLNetModel
from Bio.PDB.DSSP import DSSP
from Bio.PDB import PDBParser
import esm
import sys
sys.path.insert(0, "/home/ghd/PNBind/GAPointnet_pytorch/esm3")
from esm3.models.esm3 import ESM3
from esm3.sdk.api import ESMProtein,  LogitsConfig
from esm3.models.esmc import ESMC


res_dict = {
    'GLY': 'G', 'ALA': 'A', 'VAL': 'V', 'ILE': 'I', 'LEU': 'L', 'PHE': 'F', 'PRO': 'P', 'MET': 'M', 'TRP': 'W',
    'CYS': 'C', 'SER': 'S', 'THR': 'T', 'ASN': 'N', 'GLN': 'Q', 'TYR': 'Y', 'HIS': 'H', 'ASP': 'D', 'GLU': 'E',
    'LYS': 'K', 'ARG': 'R', 'Unknown': 'X'
}
# ======== æ–°å¢ï¼šéæ³•æ®‹åŸºä¿®å¤æ˜ å°„ ========
AA_FIX_MAP = {
    "A": "A", "C": "C", "D": "D", "E": "E", "F": "F", "G": "G", "H": "H",
    "I": "I", "K": "K", "L": "L", "M": "M", "N": "N", "P": "P", "Q": "Q",
    "R": "R", "S": "S", "T": "T", "V": "V", "W": "W", "Y": "Y",
    "U": "S", "T": "T", "G": "G", "C": "C", "A": "A",  # DNA/RNAç¢±åŸº â†’ ç±»ä¼¼æ°¨åŸºé…¸
    "X": "G", "N": "G", "-": "G", "*": "G", ".": "G", "?": "G"
}

def fix_sequence(seq_list):
    """å°†DNAç¢±åŸºæˆ–éæ³•ç¬¦å·æ˜ å°„ä¸ºåˆæ³•æ°¨åŸºé…¸ï¼Œé¿å…ESMæŠ¥é”™"""
    return [AA_FIX_MAP.get(a.upper(), "A") for a in seq_list]

pro_res_table = [
    'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X'
]

atom_type_onehot = {'N': [1, 0, 0, 0, 0], 'C': [0, 1, 0, 0, 0], 'O': [0, 0, 1, 0, 0], 'S': [0, 0, 0, 1, 0], 'H': [0, 0, 0, 0, 1]}


def one_of_k_encoding(x, allowable_set):
    if x not in allowable_set:
        x = 'X'
        raise Exception("input {0} not in allowable set{1}:".format(x, allowable_set))
    return list(map(lambda s: x == s, allowable_set))


# prottrans
def prot_xlnet_pretrain(seq, device):
    seq = ' '.join(seq)
    model_name = "Rostlab/prot_xlnet"
    if "t5" in model_name:
        tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = T5EncoderModel.from_pretrained(model_name)
    elif "albert" in model_name:
        tokenizer = AlbertTokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = AlbertModel.from_pretrained(model_name)
    elif "xlnet" in model_name:
        tokenizer = XLNetTokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = XLNetModel.from_pretrained(model_name)
    model_protein = model_protein.to(device)
    model_protein = model_protein.eval()
    fe = pipeline('feature-extraction', model=model_protein, tokenizer=tokenizer, device=device)
    embedding = fe(seq)
    embedding = np.array(embedding)
    embedding = embedding.reshape(embedding.shape[1], embedding.shape[2])
    seq_len = len(seq.replace(" ", ""))
    if "t5" in model_name:
        start_Idx = 0
        end_Idx = seq_len
        pc_feature = embedding[start_Idx:end_Idx]
    elif "albert" in model_name:
        start_Idx = 1
        end_Idx = seq_len + 1
        pc_feature = embedding[start_Idx:end_Idx]
    elif "xlnet" in model_name:
        padded_seq_len = len(embedding)
        start_Idx = padded_seq_len - seq_len - 2
        end_Idx = padded_seq_len - 2
        pc_feature = embedding[start_Idx:end_Idx]
    return pc_feature


def prot_t5_xl_bfd_pretrain(seq, device):
    seq = ' '.join(seq)
    model_name = "Rostlab/prot_t5_xl_bfd"
    if "t5" in model_name:
        tokenizer = T5Tokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = T5EncoderModel.from_pretrained(model_name)
    elif "albert" in model_name:
        tokenizer = AlbertTokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = AlbertModel.from_pretrained(model_name)
    elif "xlnet" in model_name:
        tokenizer = XLNetTokenizer.from_pretrained(model_name, do_lower_case=False)
        model_protein = XLNetModel.from_pretrained(model_name)
    model_protein = model_protein.to(device)
    model_protein = model_protein.eval()
    fe = pipeline('feature-extraction', model=model_protein, tokenizer=tokenizer, device=device)
    embedding = fe(seq)
    embedding = np.array(embedding)
    embedding = embedding.reshape(embedding.shape[1], embedding.shape[2])
    seq_len = len(seq.replace(" ", ""))
    if "t5" in model_name:
        start_Idx = 0
        end_Idx = seq_len
        pc_feature = embedding[start_Idx:end_Idx]
    elif "albert" in model_name:
        start_Idx = 1
        end_Idx = seq_len + 1
        pc_feature = embedding[start_Idx:end_Idx]
    elif "xlnet" in model_name:
        padded_seq_len = len(embedding)
        start_Idx = padded_seq_len - seq_len - 2
        end_Idx = padded_seq_len - 2
        pc_feature = embedding[start_Idx:end_Idx]
    return pc_feature


def seq_onehot(pro_seq):
    pro_hot = np.zeros((len(pro_seq), len(pro_res_table)))
    for i in range(len(pro_seq)):
        pro_hot[i, ] = one_of_k_encoding(pro_seq[i], pro_res_table)
    return pro_hot


# def get_dssp(pdb_path, len_seq):
    p = PDBParser()
    structure = p.get_structure("protein_name", pdb_path)
    model = structure[0]
    dssp = DSSP(model, pdb_path)

    secondary = []
    asa = []
    for res in dssp:
        asa.append(res[3])
        if res[2] in ('G', 'H', 'I'):
            secondary.append([1, 0, 0])
        if res[2] in ('E', 'B'):
            secondary.append([0, 1, 0])
        if res[2] in ('T', 'S', '-'):
            secondary.append([0, 0, 1])

    if len(secondary) != len_seq:
        pdb_path = pdb_path.replace('pdna_pdb', 'pdna_af2_pdb')
        p = PDBParser()
        structure = p.get_structure("protein_name", pdb_path)
        model = structure[0]
        dssp = DSSP(model, pdb_path)
        dssp_table = ['G', 'H', 'I', 'E', 'B', 'T', 'S', '-']
        secondary = []
        asa = []
        for res in dssp:
            asa.append(res[3])
            if res[2] in ('G', 'H', 'I'):
                secondary.append([1, 0, 0])
            if res[2] in ('E', 'B'):
                secondary.append([0, 1, 0])
            if res[2] in ('T', 'S', '-'):
                secondary.append([0, 0, 1])
    return secondary, asa

def cal_DSSP_prna(dssp_path, seq):
    """
    âœ… ä» DSSP æ–‡ä»¶æå– ASA ä¸äºŒçº§ç»“æ„
    âœ… å…¼å®¹å›ºå®šåˆ—å®½å’Œç©ºæ ¼åˆ†éš”æ ¼å¼
    âœ… ç”Ÿæˆ13ç»´secondaryç‰¹å¾
    """
    maxASA = {
        'A': 106, 'R': 248, 'N': 157, 'D': 163, 'C': 135, 'E': 194, 'Q': 198,
        'G': 84,  'H': 184, 'I': 169, 'L': 164, 'K': 205, 'M': 188, 'F': 197,
        'P': 136, 'S': 130, 'T': 142, 'W': 227, 'Y': 222, 'V': 142
    }
    
    # ğŸ”´ DSSP 8ç§äºŒçº§ç»“æ„ç±»å‹æ˜ å°„
    dssp_8_map = {
        ' ': 0, 'S': 1, 'T': 2, 'H': 3, 'G': 4, 
        'I': 5, 'E': 6, 'B': 7, '-': 8
    }
    
    asa_list, s2_list = [], []
    L = len(seq)

    try:
        with open(dssp_path, 'r') as f:
            lines = f.readlines()

        # ğŸ” æ›´å¥å£®çš„å¤´éƒ¨å®šä½
        start_idx = next(
            (i+1 for i, line in enumerate(lines)
             if "RESIDUE" in line and "STRUCTURE" in line),
            None
        )
        if start_idx is None:
            raise ValueError("Invalid DSSP header")

        for line in lines[start_idx:]:
            if len(line.strip()) == 0:
                continue

            aa, ss, asa_val = None, None, None

            # â‘  å›ºå®šåˆ—å®½è§£æ
            if len(line) > 38:
                try:
                    aa = line[13].strip()
                    ss = line[16].strip() if len(line) > 16 else ' '
                    asa_val = float(line[34:40].strip())
                except (ValueError, IndexError):
                    pass

            # â‘¡ split() é™çº§è§£æ
            if aa is None or asa_val is None:
                try:
                    parts = line.split()
                    if len(parts) >= 7:
                        aa = parts[2]
                        ss = parts[3] if len(parts) > 3 else ' '
                        asa_val = float(parts[6])
                except (ValueError, IndexError):
                    continue

            if not aa or aa not in maxASA:
                continue

            # ASAå½’ä¸€åŒ–
            asa_norm = min(1.0, asa_val / maxASA.get(aa, 200))
            asa_list.append(asa_norm)

            # ğŸ”´ ç”Ÿæˆ13ç»´secondaryç‰¹å¾
            ss_idx = dssp_8_map.get(ss, 0)
            ss_onehot = [0.0] * 8
            ss_onehot[ss_idx] = 1.0
            
            # æ·»åŠ 5ä¸ªå ä½ç»´åº¦ï¼ˆå¯¹é½13ç»´ï¼‰
            feature = ss_onehot + [0.0] * 5
            s2_list.append(feature)

    except Exception as e:
        print(f"[WARN] Failed to parse {dssp_path}: {e}")
        asa_list = [0.5] * L
        s2_list = [[0.0] * 13] * L  # ğŸ”´ 13ç»´é»˜è®¤å€¼

    # âœ… é•¿åº¦å¯¹é½
    if len(asa_list) < L:
        mean_asa = np.mean(asa_list) if asa_list else 0.5
        asa_list += [mean_asa] * (L - len(asa_list))
        s2_list += [[0.0] * 13] * (L - len(s2_list))  # ğŸ”´ 13ç»´
    elif len(asa_list) > L:
        asa_list = asa_list[:L]
        s2_list = s2_list[:L]

    return np.array(asa_list), np.array(s2_list)


def call_HMM(hmm_dir): 
    with open(hmm_dir, 'r') as f:
        text = f.readlines()
    hmm_begin_line = 0
    hmm_end_line = 0
    for i in range(len(text)):
        if '#' in text[i]:
            hmm_begin_line = i + 5
        elif '//' in text[i]:
            hmm_end_line = i
    hmm = np.zeros([int((hmm_end_line - hmm_begin_line) / 3), 30])

    axis_x = 0
    for i in range(hmm_begin_line, hmm_end_line, 3):
        line1 = text[i].split()[2:-1]
        line2 = text[i + 1].split()
        axis_y = 0
        for j in line1:
            if j == '*':
                hmm[axis_x][axis_y] = 9999 / 10000.0
            else:
                hmm[axis_x][axis_y] = float(j) / 10000.0
            axis_y += 1
        for j in line2:
            if j == '*':
                hmm[axis_x][axis_y] = 9999 / 10000.0
            else:
                hmm[axis_x][axis_y] = float(j) / 10000.0
            axis_y += 1
        axis_x += 1
    hmm = (hmm - np.min(hmm)) / (np.max(hmm) - np.min(hmm))

    return hmm


def process_pssm(pssm_file):
    with open(pssm_file, "r") as f:
        lines = f.readlines()
    pssm_feature = []
    for line in lines:
        if line == "\n":
            continue
        record = line.strip().split()
        if record[0].isdigit():
            pssm_feature.append([int(x) for x in record[2:22]])
    pssm_feature = np.array(pssm_feature)
    pssm_feature = (pssm_feature - np.min(pssm_feature)) / (np.max(pssm_feature) - np.min(pssm_feature))
    return pssm_feature




def rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):
    D_mu = torch.linspace(D_min, D_max, D_count)
    D_mu = D_mu.view([1, -1])
    D_sigma = (D_max - D_min) / D_count
    D_expand = torch.unsqueeze(D, -1)
    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)
    return RBF


class prna(InMemoryDataset):
    """
    PRNA æ•°æ®é›†æ„å»ºç±» â€”â€” ä½¿ç”¨ ESM3 æå–æ¯æ®‹åŸº embedding
    """
    def __init__(self, root='prna_labels', dataset='prna', data_split='train',
                 transform=None, pre_transform=None, pre_filter=None):
        self.dataset = dataset
        self.data_split = data_split
        super(prna, self).__init__(root, transform, pre_transform, pre_filter)

        if not os.path.exists(self.processed_paths[0]):
            print(f"[prna] æœªæ£€æµ‹åˆ°ç¼“å­˜ï¼Œå¼€å§‹å¤„ç†æ•°æ®é›†ï¼š{self.dataset}_{self.data_split}")
            self.process()

        self.data, self.slices = torch.load(self.processed_paths[0])
        print(f"[prna] å·²åŠ è½½å®Œæ¯•ï¼Œå…± {len(self)} ä¸ªæ ·æœ¬ ({self.dataset}_{self.data_split})")

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return [f"{self.dataset}_{self.data_split}.pt"]

    def get_neibors(self, coors, threshold=8, max_num_neighbors=32):
        return radius_graph(coors, r=threshold, max_num_neighbors=max_num_neighbors)

    def process(self):
        dssp_path = os.path.join(self.root, f"{self.dataset}_dssp")
        hmm_path  = os.path.join(self.root, f"{self.dataset}_hmm")
        pssm_path = os.path.join(self.root, f"{self.dataset}_pssm")

        all_files = os.listdir(os.path.join(self.root, f"{self.dataset}_{self.data_split}_label"))
        graph_datas = []

        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"[prna] ä½¿ç”¨è®¾å¤‡: {device}")

        for one_file in tqdm(all_files, desc=f"[prna] Processing {self.data_split}"):
            if one_file.startswith("4ne1_p"):
                print(f"[prna skip] è·³è¿‡ç¼ºå¤±æ ·æœ¬ {one_file}")
                continue

            # === è½½å…¥åæ ‡ä¸æ ‡ç­¾ ===
            with open(f"{self.root}/{self.dataset}_{self.data_split}_label_onlyc/{one_file}", 'r') as temp:
                data = [line.split() for line in temp]

            coors = [list(map(float, d[0:-2])) for d in data]
            x_features = [d[-2] for d in data]
            y = [int(d[-1]) for d in data]

            # === ä¿®å¤éæ³•æ®‹åŸº ===
            x_features = fix_sequence(x_features)

            # === æ–‡ä»¶è·¯å¾„æ£€æŸ¥ ===
            dssp_file = os.path.join(dssp_path, one_file.split('.')[0] + '.dssp')
            hmm_file  = os.path.join(hmm_path,  one_file.split('.')[0] + '.hmm')
            pssm_file = os.path.join(pssm_path, one_file.split('.')[0] + '.pssm')

            if not (os.path.exists(dssp_file) and os.path.exists(hmm_file) and os.path.exists(pssm_file)):
                print(f"[prna skip] {one_file}: ç¼ºå°‘ dssp/hmm/pssm æ–‡ä»¶")
                continue

            # === åŠ è½½æ¨¡æ€ç‰¹å¾ ===
            asa, s2 = cal_DSSP_prna(dssp_file, x_features)


            hmm     = call_HMM(hmm_file)
            pssm    = process_pssm(pssm_file)
            x_onehot = seq_onehot(x_features)
            
            # === ESM3 embeddingï¼ˆå…ˆç”Ÿæˆï¼Œåå¯¹é½ï¼‰===
            try:
                x_esm3 = esm3_sm_open(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM3 å¤±è´¥ {one_file}: {e}")
                x_esm3 = torch.zeros((len(x_features), 1536))
            # === ESMC embedding ===
            try:
                x_esmc = esmc_600m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESMC å¤±è´¥ {one_file}: {e}")
                x_esmc = torch.zeros((len(x_features), 1152))
            
            # === ESM-2 embedding (æ–°å¢) ===
            try:
                x_esm2 = esm2_t33_650m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM2 å¤±è´¥ {one_file}: {e}")
                x_esm2 = torch.zeros((len(x_features), 1280))
            # === ESM1b embedding (æ–°å¢) ===
            try:
                x_esm1b = esm1b_t33_650m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM1b å¤±è´¥ {one_file}: {e}")
                x_esm1b = torch.zeros((len(x_features), 1280))

            # === ESM2-3B embedding (æ–°å¢) ===
            try:
                x_esm2_3b = esm2_t36_3b(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM2-3B å¤±è´¥ {one_file}: {e}")
                x_esm2_3b = torch.zeros((len(x_features), 2560))
            # === å¯¹é½å„æ¨¡æ€é•¿åº¦ï¼ˆåŒ…æ‹¬ESM3ï¼‰===
            min_len = min(len(x_onehot), len(hmm), len(pssm), len(s2), len(asa), len(y), len(x_esm3), len(x_esmc),len(x_esm2), len(x_esm1b), len(x_esm2_3b))
            if min_len == 0:
                print(f"[skip] {one_file}: ç©ºç‰¹å¾ï¼Œè·³è¿‡")
                continue

            # ğŸ”´ ç»Ÿä¸€æˆªæ–­æ‰€æœ‰ç‰¹å¾
            x_onehot = x_onehot[:min_len]
            hmm = hmm[:min_len]
            pssm = pssm[:min_len]
            s2 = s2[:min_len]
            asa = asa[:min_len]
            y = y[:min_len]
            x_esm3 = x_esm3[:min_len]
            x_esmc = x_esmc[:min_len]
            x_esm2 = x_esm2[:min_len] 
            x_esm1b = x_esm1b[:min_len]    # æ–°å¢
            x_esm2_3b = x_esm2_3b[:min_len] # æ–°å¢  # ğŸ”´ å…³é”®ä¿®å¤ï¼
            # === åæ ‡é•¿åº¦å¯¹é½ ===
            len_y = len(y)
            if len(coors) != len_y:
                if len(coors) > len_y:
                    coors = coors[:len_y]
                else:
                    coors.extend([[0.0, 0.0, 0.0]] * (len_y - len(coors)))

            coors_tensor = torch.tensor(coors, dtype=torch.float)
            edge_index = self.get_neibors(coors_tensor)

            point_graph = Data(
                x=torch.tensor(x_onehot, dtype=torch.float),
                edge_index=edge_index,
                y=torch.LongTensor(y),
                hmm=torch.tensor(hmm, dtype=torch.float),
                secondary=torch.tensor(s2, dtype=torch.float),
                pssm=torch.tensor(pssm, dtype=torch.float),
                asa=torch.tensor(asa, dtype=torch.float),
                x_esm3=x_esm3,
                x_esmc=x_esmc,
                x_esm2=x_esm2,
                x_esm1b=x_esm1b,      # æ–°å¢
                x_esm2_3b=x_esm2_3b,
                pos=coors_tensor,
                protein_name=one_file
            )
            graph_datas.append(point_graph)

        # === ä¿å­˜æ•°æ® ===
        # === ä¿å­˜æ•°æ®å‰ç»Ÿä¸€è®¾å¤‡å¹¶é‡Šæ”¾æ˜¾å­˜ ===
        graph_datas = [g.to('cpu') for g in graph_datas]
        self.data, self.slices = self.collate(graph_datas)

        import gc
        gc.collect()
        torch.cuda.empty_cache()

        torch.save((self.data, self.slices), self.processed_paths[0])
        print(f"[{self.dataset}] âœ… ä¿å­˜å®Œæˆï¼Œå…± {len(graph_datas)} ä¸ªæ ·æœ¬")

        print(f"[prna] å¤„ç†å®Œæˆï¼Œå…± {len(graph_datas)} ä¸ªæ ·æœ¬ä¿å­˜åˆ° {self.processed_paths[0]}")

class pdna(InMemoryDataset):
    """
    PDNA æ•°æ®é›†æ„å»ºç±» â€”â€” ä½¿ç”¨ ESM3 æå–æ¯æ®‹åŸº embedding
    """
    def __init__(self, root='pdna_labels', dataset='pdna', data_split='train',
                 transform=None, pre_transform=None, pre_filter=None):
        self.dataset = dataset
        self.data_split = data_split
        super(pdna, self).__init__(root, transform, pre_transform, pre_filter)

        if not os.path.exists(self.processed_paths[0]):
            print(f"[pdna] æœªæ£€æµ‹åˆ°ç¼“å­˜ï¼Œå¼€å§‹å¤„ç†æ•°æ®é›†ï¼š{self.dataset}_{self.data_split}")
            self.process()

        self.data, self.slices = torch.load(self.processed_paths[0])
        print(f"[pdna] å·²åŠ è½½å®Œæ¯•ï¼Œå…± {len(self)} ä¸ªæ ·æœ¬ ({self.dataset}_{self.data_split})")

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return [f"{self.dataset}_{self.data_split}.pt"]

    def get_neibors(self, coors, threshold=8, max_num_neighbors=32):
        return radius_graph(coors, r=threshold, max_num_neighbors=max_num_neighbors)

    def process(self):
        dssp_path = os.path.join(self.root, f"{self.dataset}_dssp")
        hmm_path  = os.path.join(self.root, f"{self.dataset}_hmm")
        pssm_path = os.path.join(self.root, f"{self.dataset}_pssm")

        all_files = os.listdir(os.path.join(self.root, f"{self.dataset}_{self.data_split}_label"))
        graph_datas = []

        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"[pdna] ä½¿ç”¨è®¾å¤‡: {device}")

        for one_file in tqdm(all_files, desc=f"[pdna] Processing {self.data_split}"):
            if one_file.startswith("4ne1_p"):
                print(f"[pdna skip] è·³è¿‡ç¼ºå¤±æ ·æœ¬ {one_file}")
                continue

            with open(f"{self.root}/{self.dataset}_{self.data_split}_label_onlyc/{one_file}", 'r') as temp:
                data = [line.split() for line in temp]

            coors = [list(map(float, d[0:-2])) for d in data]
            x_features = [d[-2] for d in data]
            y = [int(d[-1]) for d in data]

            x_features = fix_sequence(x_features)

            dssp_file = os.path.join(dssp_path, one_file.split('.')[0] + '.dssp')
            hmm_file  = os.path.join(hmm_path,  one_file.split('.')[0] + '.hmm')
            pssm_file = os.path.join(pssm_path, one_file.split('.')[0] + '.pssm')

            if not (os.path.exists(dssp_file) and os.path.exists(hmm_file) and os.path.exists(pssm_file)):
                print(f"[pdna skip] {one_file}: ç¼ºå°‘ dssp/hmm/pssm æ–‡ä»¶")
                continue

            asa, s2 = cal_DSSP_prna(dssp_file, x_features)


            hmm     = call_HMM(hmm_file)
            pssm    = process_pssm(pssm_file)
            x_onehot = seq_onehot(x_features)
            # === ESM3 embeddingï¼ˆå…ˆç”Ÿæˆï¼Œåå¯¹é½ï¼‰===
            try:
                x_esm3 = esm3_sm_open(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM3 å¤±è´¥ {one_file}: {e}")
                x_esm3 = torch.zeros((len(x_features), 1536))
            # === ESMC embedding ===
            try:
                x_esmc = esmc_600m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESMC å¤±è´¥ {one_file}: {e}")
                x_esmc = torch.zeros((len(x_features), 1152))
            # === ESM-2 embedding (æ–°å¢) ===
            try:
                x_esm2 = esm2_t33_650m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM2 å¤±è´¥ {one_file}: {e}")
                x_esm2 = torch.zeros((len(x_features), 1280))
            # === ESM1b embedding (æ–°å¢) ===
            try:
                x_esm1b = esm1b_t33_650m(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM1b å¤±è´¥ {one_file}: {e}")
                x_esm1b = torch.zeros((len(x_features), 1280))

            # === ESM2-3B embedding (æ–°å¢) ===
            try:
                x_esm2_3b = esm2_t36_3b(x_features, device=device)
            except Exception as e:
                print(f"[warn] ESM2-3B å¤±è´¥ {one_file}: {e}")
                x_esm2_3b = torch.zeros((len(x_features), 2560))
            # === å¯¹é½å„æ¨¡æ€é•¿åº¦ï¼ˆåŒ…æ‹¬ESM3ï¼‰===
            min_len = min(len(x_onehot), len(hmm), len(pssm), len(s2), len(asa), len(y), len(x_esm3),len(x_esmc), len(x_esm2), len(x_esm1b), len(x_esm2_3b))
            if min_len == 0:
                print(f"[skip] {one_file}: ç©ºç‰¹å¾ï¼Œè·³è¿‡")
                continue

            # ğŸ”´ ç»Ÿä¸€æˆªæ–­æ‰€æœ‰ç‰¹å¾
            x_onehot = x_onehot[:min_len]
            hmm = hmm[:min_len]
            pssm = pssm[:min_len]
            s2 = s2[:min_len]
            asa = asa[:min_len]
            y = y[:min_len]
            x_esm3 = x_esm3[:min_len]  # ğŸ”´ å…³é”®ä¿®å¤ï¼
            x_esmc = x_esmc[:min_len]
            x_esm2 = x_esm2[:min_len]
            x_esm1b = x_esm1b[:min_len]    # æ–°å¢
            x_esm2_3b = x_esm2_3b[:min_len] # æ–°å¢
            len_y = len(y)
            if len(coors) != len_y:
                if len(coors) > len_y:
                    coors = coors[:len_y]
                else:
                    coors.extend([[0.0, 0.0, 0.0]] * (len_y - len(coors)))

            coors_tensor = torch.tensor(coors, dtype=torch.float)
            edge_index = self.get_neibors(coors_tensor)

            point_graph = Data(
                x=torch.tensor(x_onehot, dtype=torch.float),
                edge_index=edge_index,
                y=torch.LongTensor(y),
                hmm=torch.tensor(hmm, dtype=torch.float),
                secondary=torch.tensor(s2, dtype=torch.float),
                pssm=torch.tensor(pssm, dtype=torch.float),
                asa=torch.tensor(asa, dtype=torch.float),
                x_esm3=x_esm3,
                x_esmc=x_esmc,
                x_esm2=x_esm2,
                x_esm1b=x_esm1b,      # æ–°å¢
                x_esm2_3b=x_esm2_3b,  # æ–°å¢
                pos=coors_tensor,
                protein_name=one_file
            )
            graph_datas.append(point_graph)

        self.data, self.slices = self.collate(graph_datas)
        torch.save((self.data, self.slices), self.processed_paths[0])
        print(f"[pdna] å¤„ç†å®Œæˆï¼Œå…± {len(graph_datas)} ä¸ªæ ·æœ¬ä¿å­˜åˆ° {self.processed_paths[0]}")

def esm3_sm_open(seq, device='cpu'):
    """
    ä» ESM3-sm-open-v1 æå–æ¯ä¸ªæ°¨åŸºé…¸çš„ 1536ç»´ embeddingã€‚
    âœ… ä¿®å¤ï¼šä¸å†è§¦å‘ç»“æ„æ„ŸçŸ¥æ¨¡å¼ï¼ˆé¿å…æ¯åŸå­ embeddingï¼‰
    âœ… å¢å¼ºï¼šå…¨å±€ z-score å½’ä¸€åŒ– + clamp(-5,5) é™å¹…
    âœ… è¾“å‡º shape = [L, 1536], å·²æ ‡å‡†åŒ–ä¸”æ— æ˜¾å­˜æ³„æ¼
    """
    import torch
    import torch.nn.functional as F
    import gc
    from esm3.models.esm3 import ESM3
    from esm3.sdk.api import get_esm3_model_tokenizers

    global _esm3_model_cache
    if not isinstance(seq, (list, tuple)):
        seq = list(seq)

    legal_aa = set("ACDEFGHIKLMNPQRSTVWY")
    seq_fixed = ['A' if aa.upper() not in legal_aa else aa.upper() for aa in seq]
    seq_str = ''.join(seq_fixed)

    try:
        # 1ï¸âƒ£ æ¨¡å‹ç¼“å­˜ä¸åŠ è½½
        if _esm3_model_cache is None:
            print(f"[ESM3] Loading esm3_sm_open_v1 model ...")
            model = ESM3.from_pretrained("/home/ghd/PNBind/GAPointnet_pytorch/esm3/model")
            model.tokenizers = get_esm3_model_tokenizers()
            model = model.float()
            _esm3_model_cache = model
            print(f"[ESM3] âœ… Model cached successfully")

        model = _esm3_model_cache.to(device)
        model.eval()

        # 2ï¸âƒ£ ç¼–ç ä¸æ¨ç†
        tokens = model.tokenizers.sequence.encode(seq_str, add_special_tokens=True)
        tokens_tensor = torch.tensor([tokens], device=device)

        with torch.no_grad():
            output = model(sequence_tokens=tokens_tensor)

        # 3ï¸âƒ£ æå– embeddingï¼ˆå»æ‰BOS/EOSï¼‰
        emb = output.embeddings[0, 1:-1, :]
        emb = F.layer_norm(emb, emb.shape[1:])  # å±€éƒ¨å½’ä¸€åŒ–ï¼ˆæ¯æ®‹åŸºå†…éƒ¨ï¼‰

        # 4ï¸âƒ£ å…¨å±€æ ‡å‡†åŒ– + clamp
        mean = emb.mean()
        std = emb.std()
        emb = (emb - mean) / (std + 1e-6)
        emb = torch.clamp(emb, -5, 5)

        # 5ï¸âƒ£ æ˜¾å­˜æ¸…ç†
        emb = emb.detach().cpu()
        del output, tokens_tensor
        torch.cuda.empty_cache()
        gc.collect()

        return emb

    except Exception as e:
        print(f"[ESM3] âŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        torch.cuda.empty_cache()
        gc.collect()
        return torch.zeros((len(seq_fixed), 1536))

def esmc_600m(seq, device='cpu'):
    """
    ä» ESMC-600M æå–æ¯ä¸ªæ°¨åŸºé…¸çš„ embedding (1152ç»´)
    âœ… ä»æœ¬åœ°æƒé‡æ–‡ä»¶åŠ è½½
    âœ… å…¨å±€æ¨¡å‹ç¼“å­˜
    âœ… å½’ä¸€åŒ–å¤„ç†
    """
    import torch
    import torch.nn.functional as F
    import gc

    global _esmc_model_cache
    
    if not isinstance(seq, (list, tuple)):
        seq = list(seq)

    legal_aa = set("ACDEFGHIKLMNPQRSTVWY")
    seq_fixed = ['A' if aa.upper() not in legal_aa else aa.upper() for aa in seq]
    seq_str = ''.join(seq_fixed)

    try:
        # 1ï¸âƒ£ æ¨¡å‹ç¼“å­˜ä¸åŠ è½½
        if _esmc_model_cache is None:
            print(f"[ESMC] Loading esmc_600m model from local weights...")
            
            # ğŸ”´ ä»ä½ çš„æœ¬åœ°æƒé‡æ–‡ä»¶åŠ è½½
            model_path = "/home/ghd/PNBind/GAPointnet_pytorch/esm3/data/weights/esmc_600m_2024_12_v0.pth"
            
            # åŠ è½½æ¨¡å‹
            model = ESMC.from_pretrained("esmc_600m")  # å…ˆåŠ è½½ç»“æ„
            state_dict = torch.load(model_path, map_location='cpu')
            model.load_state_dict(state_dict)
            model = model.float()
            
            _esmc_model_cache = model
            print(f"[ESMC] âœ… Model cached successfully from {model_path}")

        model = _esmc_model_cache.to(device)
        model.eval()

        # 2ï¸âƒ£ ç¼–ç ä¸æ¨ç†
        protein = ESMProtein(sequence=seq_str)
        protein_tensor = model.encode(protein)
        
        with torch.no_grad():
            logits_output = model.logits(
                protein_tensor, 
                LogitsConfig(sequence=True, return_embeddings=True)
            )

        # 3ï¸âƒ£ æå– embeddingï¼ˆå»æ‰BOS/EOSï¼‰
        emb = logits_output.embeddings.squeeze(0)
        if len(emb) > len(seq_str):
            emb = emb[1:len(seq_str)+1]  # å»æ‰ç‰¹æ®Štoken
        else:
            emb = emb[:len(seq_str)]
        
        # 4ï¸âƒ£ å½’ä¸€åŒ–å¤„ç†ï¼ˆä¸ESM3ä¿æŒä¸€è‡´ï¼‰
        emb = F.layer_norm(emb, emb.shape[1:])
        mean = emb.mean()
        std = emb.std()
        emb = (emb - mean) / (std + 1e-6)
        emb = torch.clamp(emb, -10, 10)

        # 5ï¸âƒ£ æ˜¾å­˜æ¸…ç†
        emb = emb.detach().cpu()
        del logits_output, protein_tensor
        torch.cuda.empty_cache()
        gc.collect()

        return emb

    except Exception as e:
        print(f"[ESMC] âŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        torch.cuda.empty_cache()
        gc.collect()
        return torch.zeros((len(seq_fixed), 1152))  # ESMCæ˜¯1152ç»´

def esm2_t33_650m(seq, device='cpu'):
    """
    ä»æœ¬åœ°æƒé‡åŠ è½½ ESM-2 t33 650Mï¼Œæå–1280ç»´embedding
    """
    import torch
    import torch.nn.functional as F
    import gc
    import esm
    from esm.pretrained import load_model_and_alphabet_core
    
    global _esm2_model_cache, _esm2_alphabet_cache
    
    if not isinstance(seq, (list, tuple)):
        seq = list(seq)
    
    legal_aa = set("ACDEFGHIKLMNPQRSTVWY")
    seq_fixed = ['A' if aa.upper() not in legal_aa else aa.upper() for aa in seq]
        # ğŸ”´ æ–°å¢ï¼šæˆªæ–­è¶…é•¿åºåˆ—
    original_len = len(seq_fixed)
    MAX_LEN = 1022
    if original_len > MAX_LEN:
        seq_fixed = seq_fixed[:MAX_LEN]
        print(f"[ESM2] Truncated {original_len} -> {MAX_LEN}")
    seq_str = ''.join(seq_fixed)
    
    try:
        # 1ï¸âƒ£ æ¨¡å‹ç¼“å­˜ä¸åŠ è½½
        if _esm2_model_cache is None:
            print(f"[ESM2] Loading esm2_t33_650M from local weights...")
            model_path = "/home/ghd/PNBind/GAPointnet_pytorch/esm3/data/weights/esm2_t33_650M_UR50D.pt"
            model_data = torch.load(model_path, map_location='cpu')
            model, alphabet = load_model_and_alphabet_core("esm2_t33_650M_UR50D", model_data, None)
            model = model.float()
            _esm2_model_cache = model
            _esm2_alphabet_cache = alphabet
            print(f"[ESM2] âœ… Model cached (1280-dim, 651M params)")
        
        model = _esm2_model_cache.to(device)
        alphabet = _esm2_alphabet_cache
        batch_converter = alphabet.get_batch_converter()
        model.eval()
        
        # 2ï¸âƒ£ ç¼–ç ä¸æ¨ç†
        data = [("protein", seq_str)]
        batch_labels, batch_strs, batch_tokens = batch_converter(data)
        batch_tokens = batch_tokens.to(device)
        
        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[33], return_contacts=False)
        
        # 3ï¸âƒ£ æå–embeddingï¼ˆå»æ‰BOS/EOSï¼‰
        emb = results["representations"][33][0, 1:-1, :]  # [L, 1280]
        
        # 4ï¸âƒ£ å½’ä¸€åŒ–å¤„ç†
        emb = F.layer_norm(emb, emb.shape[1:])
        mean = emb.mean()
        std = emb.std()
        emb = (emb - mean) / (std + 1e-6)
        emb = torch.clamp(emb, -5, 5)
        
        # 5ï¸âƒ£ æ˜¾å­˜æ¸…ç†
        emb = emb.detach().cpu()
        del results, batch_tokens
        torch.cuda.empty_cache()
        gc.collect()
        
        return emb
    
    except Exception as e:
        print(f"[ESM2] âŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        torch.cuda.empty_cache()
        gc.collect()
        return torch.zeros((len(seq_fixed), 1280))

def esm1b_t33_650m(seq, device='cpu'):
    """
    ä»æœ¬åœ°æƒé‡åŠ è½½ ESM-1b t33 650Mï¼Œæå–1280ç»´embedding
    """
    import torch
    import torch.nn.functional as F
    import gc
    import esm
    from esm.pretrained import load_model_and_alphabet_core
    
    global _esm1b_model_cache, _esm1b_alphabet_cache
    
    if not isinstance(seq, (list, tuple)):
        seq = list(seq)
    
    legal_aa = set("ACDEFGHIKLMNPQRSTVWY")
    seq_fixed = ['A' if aa.upper() not in legal_aa else aa.upper() for aa in seq]
    original_len = len(seq_fixed)
    MAX_LEN = 1022
    if original_len > MAX_LEN:
        seq_fixed = seq_fixed[:MAX_LEN]
        print(f"[ESM1b] Truncated {original_len} -> {MAX_LEN}")
    
    seq_str = ''.join(seq_fixed)
    seq_str = ''.join(seq_fixed)
    
    try:
        # 1ï¸âƒ£ æ¨¡å‹ç¼“å­˜ä¸åŠ è½½
        if _esm1b_model_cache is None:
            print(f"[ESM1b] Loading esm1b_t33_650M from local weights...")
            model_path = "/home/ghd/PNBind/GAPointnet_pytorch/esm3/data/weights/esm1b_t33_650M_UR50S.pt"
            model_data = torch.load(model_path, map_location='cpu')
            model, alphabet = load_model_and_alphabet_core("esm1b_t33_650M_UR50S", model_data, None)
            model = model.float()
            _esm1b_model_cache = model
            _esm1b_alphabet_cache = alphabet
            print(f"[ESM1b] âœ… Model cached (1280-dim)")
        
        model = _esm1b_model_cache.to(device)
        alphabet = _esm1b_alphabet_cache
        batch_converter = alphabet.get_batch_converter()
        model.eval()
        
        # 2ï¸âƒ£ ç¼–ç ä¸æ¨ç†
        data = [("protein", seq_str)]
        batch_labels, batch_strs, batch_tokens = batch_converter(data)
        batch_tokens = batch_tokens.to(device)
        
        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[33], return_contacts=False)
        
        # 3ï¸âƒ£ æå–embeddingï¼ˆå»æ‰BOS/EOSï¼‰
        emb = results["representations"][33][0, 1:-1, :]  # [L, 1280]
        
        # 4ï¸âƒ£ å½’ä¸€åŒ–å¤„ç†
        emb = F.layer_norm(emb, emb.shape[1:])
        mean = emb.mean()
        std = emb.std()
        emb = (emb - mean) / (std + 1e-6)
        emb = torch.clamp(emb, -5, 5)
        
        # 5ï¸âƒ£ æ˜¾å­˜æ¸…ç†
        emb = emb.detach().cpu()
        del results, batch_tokens
        torch.cuda.empty_cache()
        gc.collect()
        
        return emb
    
    except Exception as e:
        print(f"[ESM1b] âŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        torch.cuda.empty_cache()
        gc.collect()
        return torch.zeros((len(seq_fixed), 1280))

def esm2_t36_3b(seq, device='cpu'):
    """
    ä»æœ¬åœ°æƒé‡åŠ è½½ ESM-2 t36 3Bï¼Œæå–2560ç»´embedding
    """
    import torch
    import torch.nn.functional as F
    import gc
    import esm
    from esm.pretrained import load_model_and_alphabet_core
    
    global _esm2_3b_model_cache, _esm2_3b_alphabet_cache
    
    if not isinstance(seq, (list, tuple)):
        seq = list(seq)
    
    legal_aa = set("ACDEFGHIKLMNPQRSTVWY")
    seq_fixed = ['A' if aa.upper() not in legal_aa else aa.upper() for aa in seq]
        # ğŸ”´ æ–°å¢ï¼šæˆªæ–­è¶…é•¿åºåˆ—ï¼ˆ3Bæ¨¡å‹æ”¯æŒæ›´é•¿ï¼‰
    original_len = len(seq_fixed)
    MAX_LEN = 2046
    if original_len > MAX_LEN:
        seq_fixed = seq_fixed[:MAX_LEN]
        print(f"[ESM2-3B] Truncated {original_len} -> {MAX_LEN}")
    seq_str = ''.join(seq_fixed)
    
    try:
        # 1ï¸âƒ£ æ¨¡å‹ç¼“å­˜ä¸åŠ è½½
        if _esm2_3b_model_cache is None:
            print(f"[ESM2-3B] Loading esm2_t36_3B from local weights...")
            model_path = "/home/ghd/PNBind/GAPointnet_pytorch/esm3/data/weights/esm2_t36_3B_UR50D.pt"
            model_data = torch.load(model_path, map_location='cpu')
            model, alphabet = load_model_and_alphabet_core("esm2_t36_3B_UR50D", model_data, None)
            model = model.float()
            _esm2_3b_model_cache = model
            _esm2_3b_alphabet_cache = alphabet
            print(f"[ESM2-3B] âœ… Model cached (2560-dim, 3B params)")
        
        model = _esm2_3b_model_cache.to(device)
        alphabet = _esm2_3b_alphabet_cache
        batch_converter = alphabet.get_batch_converter()
        model.eval()
        
        # 2ï¸âƒ£ ç¼–ç ä¸æ¨ç†
        data = [("protein", seq_str)]
        batch_labels, batch_strs, batch_tokens = batch_converter(data)
        batch_tokens = batch_tokens.to(device)
        
        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[36], return_contacts=False)
        
        # 3ï¸âƒ£ æå–embeddingï¼ˆå»æ‰BOS/EOSï¼‰
        emb = results["representations"][36][0, 1:-1, :]  # [L, 2560]
        
        # 4ï¸âƒ£ å½’ä¸€åŒ–å¤„ç†
        emb = F.layer_norm(emb, emb.shape[1:])
        mean = emb.mean()
        std = emb.std()
        emb = (emb - mean) / (std + 1e-6)
        emb = torch.clamp(emb, -5, 5)
        
        # 5ï¸âƒ£ æ˜¾å­˜æ¸…ç†
        emb = emb.detach().cpu()
        del results, batch_tokens
        torch.cuda.empty_cache()
        gc.collect()
        
        return emb
    
    except Exception as e:
        print(f"[ESM2-3B] âŒ Failed: {e}")
        import traceback
        traceback.print_exc()
        torch.cuda.empty_cache()
        gc.collect()
        return torch.zeros((len(seq_fixed), 2560))   
if __name__ == "__main__":
    lengths = analyze_sequence_lengths()
    configs = [
        ("prna_labels", "prna", "train"),
        ("prna_labels", "prna", "test"),
        ("pdna_labels", "pdna", "train"),
        ("pdna_labels", "pdna", "test"),
    ]

    for root, dataset, split in configs:
        print(f"\n=== å¼€å§‹ç”Ÿæˆ {dataset}_{split}.pt ===")
        if dataset == "prna":
            ds = prna(root=root, dataset=dataset, data_split=split)
        else:
            ds = pdna(root=root, dataset=dataset, data_split=split)
        print(f"{dataset}_{split} æ ·æœ¬æ•°: {len(ds)}")

